{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5X1d9FnfT40o87ALdeLgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveenkumark1/Extensive_Vision_and_AI_V.4/blob/master/Week2/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7X8JznTOrFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch                                      ## Import torch\n",
        "import torchvision                                ## Import torch vision for vision related data usage\n",
        "from torchvision import transforms, datasets      ## Import from torch vision importing ransforms and data sets for our use "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lrnt3__O9mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Getting the data \n",
        "# Trianing Data  : MNIST 28 X 28\n",
        "\n",
        "train =  datasets.MNIST(\"\",train= True, download= True, transform = transforms.Compose([transforms.ToTensor()]))\n",
        "# variable = datasets.MNIST(\"\" = keep data in local,\n",
        "            # train = Consider the data as a trinaing set \n",
        "            # download = Download the data fromt the internet and keep in local\n",
        "            # transform = Data is not natively in the form of TENSOR, So to convert in to tensor we use this, \n",
        "            # transform the PIL format image to a transformed verison ) # Same is avaiable in documentaiton \n",
        "\n",
        "\n",
        "# Insample data & out sample data !! -> data for training and dat afor testing \n",
        "\n",
        "# Testing data \n",
        "\n",
        "test = datasets.MNIST(\"\",train= False, download= True, transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg_7SO2SUrof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will extract the data that is downloaded in to the train by using the \" torch.util.dataloader \"\n",
        "\n",
        "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle= True)\n",
        "\n",
        "# trainset = torch.utils.data.DDDDataLLLLoader(source, batch size , shuffle )\n",
        "\n",
        "# source = From where the data to be loaded \n",
        "# batch_size = How many of the items we need to pass through in one time to the network\n",
        "# shuffle = Generalisaiton (We feed data randomly so that the network can recognise the digits without any bias)\n",
        "\n",
        "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9igw9TXAwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "outputId": "8822591c-bd60-4248-9917-2e775071aafc"
      },
      "source": [
        "## Itereate over the data\n",
        "\n",
        "for data in trainset:\n",
        "  print(data)\n",
        "  break\n",
        "\n",
        "# This data will have a tensor of tensors - Images  and also tensor of tensors that are labels "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 4, 4, 1, 3, 1, 6, 6, 5, 2])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xopai3pzgb6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "outputId": "059c620d-dc19-4198-e6c3-c4d4717005e7"
      },
      "source": [
        "data[0]  ## Images "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEbvetMVjo_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3eb31122-0d4e-453b-cc36-3fd40a70f56b"
      },
      "source": [
        "data[1]  # Labels"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 4, 4, 1, 3, 1, 6, 6, 5, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWw99c5Cjuht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = data[0][0], data[1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHTGpEpWj3X4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd7a38b4-c389-45f9-e488-71def9a5ab30"
      },
      "source": [
        "x"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490,\n",
              "          0.7490, 0.4431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7725,\n",
              "          0.9882, 0.9882, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3451,\n",
              "          0.9882, 0.9882, 0.8980, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
              "          0.9490, 0.9882, 0.6980, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.6627, 0.9882, 0.7961, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5765, 0.8706, 0.1490, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.6627, 0.9922, 1.0000, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.8824, 0.9882, 0.5451, 0.1490,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.6627, 0.9882, 0.9922, 0.6549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.4706, 0.9647, 0.9882, 0.9882, 0.8667,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.5373, 0.9882, 0.9922, 0.9137, 0.1686, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.9294, 0.9882, 0.8392, 0.3882, 0.8745,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0392, 0.6941, 0.9922, 0.9882, 0.5843, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.4431, 0.9882, 0.7647, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.2471, 0.9922, 0.9882, 0.7647, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.4471, 0.9922, 0.9922, 0.3294, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.2980, 0.7490, 1.0000, 0.9922, 0.8706, 0.1490, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.2941, 0.9529, 0.9882, 0.5451, 0.1490,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.1882, 0.3333, 0.5137, 0.8824,\n",
              "          0.9529, 0.9882, 0.9922, 0.9882, 0.9882, 0.3294, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4549, 0.9882, 0.9882, 0.8667,\n",
              "          0.7765, 0.7725, 0.7725, 0.7725, 0.8902, 0.9922, 0.9882, 0.9882,\n",
              "          0.9882, 0.9882, 0.8706, 0.9882, 0.9882, 0.3294, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.7804, 0.9882, 0.9882,\n",
              "          0.9922, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9490, 0.7529,\n",
              "          0.3294, 0.3294, 0.1490, 0.9176, 0.9882, 0.3882, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.5020, 0.9882,\n",
              "          0.9922, 0.9882, 0.8627, 0.4392, 0.4392, 0.4392, 0.2902, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.8824, 0.9882, 0.8745, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.8863, 0.9922, 0.9922, 0.4392, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.3922, 0.9882, 0.9882, 0.9294, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.3333, 0.9882, 0.9882, 0.9882, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.1098, 0.7804, 0.9765, 0.8745, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.3882, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstJioOLj5mZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "667a38e1-99fa-4965-91d6-0ad90dbdba15"
      },
      "source": [
        "y"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPxXUzZhj-kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Printing the images to know if we are correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sETuqBwAkBxV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "6dea1594-29c3-4ccf-93b9-24540ccb4af0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(data[0][0])\n",
        "\n",
        "# Shape mismatch (1,28,28) - Normal format is 28X28 - pytorch format 1X28X28"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1e2e1f231e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Shape mismatch (1,28,28) - Normal format is 28X28 - pytorch format 1X28X28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2677\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2678\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 28, 28) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhkFSYxgkNYZBUmMYJDX/AwqkUdV2nfELAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4TjLRZPlf-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea0d77a3-b9ec-4d71-c902-17fbeeca233d"
      },
      "source": [
        "print(data[0][0].shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnsRPvI9lDhu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80a7ac7b-104d-45aa-d053-c1749d8a7698"
      },
      "source": [
        "print(data[0][0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490,\n",
            "          0.7490, 0.4431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7725,\n",
            "          0.9882, 0.9882, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3451,\n",
            "          0.9882, 0.9882, 0.8980, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
            "          0.9490, 0.9882, 0.6980, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.6627, 0.9882, 0.7961, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5765, 0.8706, 0.1490, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.6627, 0.9922, 1.0000, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.8824, 0.9882, 0.5451, 0.1490,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.6627, 0.9882, 0.9922, 0.6549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.4706, 0.9647, 0.9882, 0.9882, 0.8667,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.5373, 0.9882, 0.9922, 0.9137, 0.1686, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.9294, 0.9882, 0.8392, 0.3882, 0.8745,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0392, 0.6941, 0.9922, 0.9882, 0.5843, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.4431, 0.9882, 0.7647, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2471, 0.9922, 0.9882, 0.7647, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.4471, 0.9922, 0.9922, 0.3294, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.2980, 0.7490, 1.0000, 0.9922, 0.8706, 0.1490, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.2941, 0.9529, 0.9882, 0.5451, 0.1490,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1882, 0.3333, 0.5137, 0.8824,\n",
            "          0.9529, 0.9882, 0.9922, 0.9882, 0.9882, 0.3294, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4549, 0.9882, 0.9882, 0.8667,\n",
            "          0.7765, 0.7725, 0.7725, 0.7725, 0.8902, 0.9922, 0.9882, 0.9882,\n",
            "          0.9882, 0.9882, 0.8706, 0.9882, 0.9882, 0.3294, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.7804, 0.9882, 0.9882,\n",
            "          0.9922, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9490, 0.7529,\n",
            "          0.3294, 0.3294, 0.1490, 0.9176, 0.9882, 0.3882, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.5020, 0.9882,\n",
            "          0.9922, 0.9882, 0.8627, 0.4392, 0.4392, 0.4392, 0.2902, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.8824, 0.9882, 0.8745, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.8863, 0.9922, 0.9922, 0.4392, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.3922, 0.9882, 0.9882, 0.9294, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.3333, 0.9882, 0.9882, 0.9882, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.1098, 0.7804, 0.9765, 0.8745, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.3882, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Jykr6nlwjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "683f11d8-6a7d-46a7-a74a-b69cf29b5993"
      },
      "source": [
        "## View function in pytorch \"\" view(28,28) \"\"\n",
        "\n",
        "print(data[0][0].shape)\n",
        "plt.imshow(data[0][0].view(28,28))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc685fa5748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGVflKwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIjo5KuLmjxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Balancing \n",
        "\n",
        "## Find the count of the every number in MNIST and also the percentage "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf20WbLw2ThU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55d0ab1c-9b10-47a6-e3d8-63256c34ebef"
      },
      "source": [
        "total = 0 \n",
        "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "\n",
        "for data in trainset:\n",
        "  xs, ys = data\n",
        "  for y in ys:\n",
        "    counter_dict[int(y)] += 1\n",
        "    total+=1\n",
        "    \n",
        "print(counter_dict)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX5hwwYF4JOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "9690fd09-6753-467a-8227-e7ac236a8652"
      },
      "source": [
        "for i in counter_dict:\n",
        "  print(i,counter_dict[i]/total*100)\n",
        "\n",
        "  # Looks balanced !!"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 9.871666666666666\n",
            "1 11.236666666666666\n",
            "2 9.93\n",
            "3 10.218333333333334\n",
            "4 9.736666666666666\n",
            "5 9.035\n",
            "6 9.863333333333333\n",
            "7 10.441666666666666\n",
            "8 9.751666666666667\n",
            "9 9.915000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9Cfe5V4hRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Lets build the Neural network and see !!\n",
        "\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F  ## This has the activaion functions\n",
        "\n",
        " ## torch.nn is more or object oriented programming and torch.functional is basically functions to run a single function \n",
        "\n",
        " ## Functions - Pass parameter\n",
        " ## NN - Initialise things "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsLS4yCN7TGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "62c75fd1-d3e1-44fe-f590-b5eff449a9df"
      },
      "source": [
        "## Just a Network \n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):  ## Intialise the module Self\n",
        "    super().__init__() ## Initialisation to inherit the methods and (attributes?) from the parent class  -- Explore OOP  -- Super init error\n",
        "    ## Defining the fully connected layer \n",
        "\n",
        "    self.fc1 = nn.Linear(in_features= 28*28, out_features= 64, bias= True) \n",
        "   # planning to get 3 layers of 64 neurons in a layer - > output  \n",
        "   # self.fc1 = nn.Linear(FEATURES TO INPUT -> size of the image - flattened 28*28 or 784), FEATURES COMES IN OUTPUT -> 64 in this case, bias= EXPLORE)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 64)\n",
        "    self.fc4 = nn.Linear(64, 10)  # Note : Output is 10 - Because in MNIST the total output we have 10 classes\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSbYaLuJM65g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We have defined a Neural network above, But we do not have the path for the data to pass through the network\n",
        "## We will define a feed forward neural network here - The info passes in only one direction \n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):  ## Intialise the module Self\n",
        "    super().__init__() ## Initialisation to inherit the methods and (attributes?) from the parent class  -- Explore OOP  -- Super init error\n",
        "    ## Defining the fully connected layer \n",
        "\n",
        "    self.fc1 = nn.Linear(in_features= 28*28, out_features= 64, bias= True) \n",
        "   # planning to get 3 layers of 64 neurons in a layer - > output  \n",
        "   # self.fc1 = nn.Linear(FEATURES TO INPUT -> size of the image - flattened 28*28 or 784), FEATURES COMES IN OUTPUT -> 64 in this case, bias= EXPLORE)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 64)\n",
        "    self.fc4 = nn.Linear(64, 10)  # Note : Output is 10 - Because in MNIST the total output we have 10 classes\n",
        "\n",
        "### FEED FORWARD NETWORK \n",
        "\n",
        "  def forward(self,x): ## Define a forward path for 'self' with input 'x'\n",
        "    x = self.fc1(x)  # Sending the data 'x' to fc1 and similarly through all others.\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.fc4(x)\n",
        "\n",
        "    return(x)\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "### THIS NETWORK WILL NOT LEARN ANYTHING : The activaiton function is missing \n",
        "# Below we will write a network with the activation function also. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k6kE_gOO1Zd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0776420d-53f4-42c8-c91f-be8242c69289"
      },
      "source": [
        "## NETWORK WITH ACTIVATION FUNCTION\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):  ## Intialise the module Self\n",
        "    super().__init__() ## Initialisation to inherit the methods and (attributes?) from the parent class  -- Explore OOP  -- Super init error\n",
        "    ## Defining the fully connected layer \n",
        "\n",
        "    self.fc1 = nn.Linear(in_features= 28*28, out_features= 64, bias= True) \n",
        "   # planning to get 3 layers of 64 neurons in a layer - > output  \n",
        "   # self.fc1 = nn.Linear(FEATURES TO INPUT -> size of the image - flattened 28*28 or 784), FEATURES COMES IN OUTPUT -> 64 in this case, bias= EXPLORE)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 64)\n",
        "    self.fc4 = nn.Linear(64, 10)  # Note : Output is 10 - Because in MNIST the total output we have 10 classes\n",
        "\n",
        "### FEED FORWARD NETWORK \n",
        "\n",
        "  def forward(self,x): ## Define a forward path for 'self' with input 'x'\n",
        "    x = F.relu(self.fc1(x))  \n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)               ### NO Activaiton funciton on the last  - Here we want only one class to be fired which gives the output. \n",
        "\n",
        "    ## To get a log probability distribution we will use 'softmax' as an optimizer in the last layer \n",
        "    return F.log_softmax(x, dim = 1)  ## We want the probability of all the classes to sum to one.- EXPLORE\n",
        "\n",
        "net = Net()\n",
        "print(net)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPNv1nHLQo_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.rand((28,28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCtzDWVCQw7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c0110bb-485e-46fc-8324-26508ff33b0b"
      },
      "source": [
        "X"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7635, 0.7088, 0.4113, 0.1145, 0.1896, 0.8356, 0.0997, 0.8342, 0.8903,\n",
              "         0.9280, 0.3596, 0.7090, 0.7479, 0.1718, 0.0388, 0.1297, 0.6318, 0.1872,\n",
              "         0.5006, 0.3358, 0.1253, 0.3319, 0.9166, 0.9828, 0.3388, 0.2563, 0.2164,\n",
              "         0.6153],\n",
              "        [0.6127, 0.2032, 0.9707, 0.0861, 0.9155, 0.3231, 0.7973, 0.9530, 0.4179,\n",
              "         0.6897, 0.8548, 0.6798, 0.3921, 0.7980, 0.6558, 0.7185, 0.2962, 0.3071,\n",
              "         0.7749, 0.2593, 0.4806, 0.6221, 0.1464, 0.7246, 0.8728, 0.9227, 0.9156,\n",
              "         0.3789],\n",
              "        [0.2387, 0.0612, 0.8055, 0.0519, 0.2604, 0.9657, 0.5302, 0.5959, 0.3994,\n",
              "         0.2450, 0.7670, 0.0162, 0.2198, 0.2025, 0.7002, 0.2072, 0.2420, 0.2213,\n",
              "         0.7523, 0.8604, 0.3231, 0.2199, 0.7482, 0.5591, 0.2719, 0.3746, 0.0030,\n",
              "         0.4485],\n",
              "        [0.9301, 0.8209, 0.2644, 0.6305, 0.3242, 0.2553, 0.9679, 0.4821, 0.2743,\n",
              "         0.7417, 0.8591, 0.9978, 0.8940, 0.4513, 0.0890, 0.6578, 0.6187, 0.2536,\n",
              "         0.4115, 0.7572, 0.8396, 0.6955, 0.4532, 0.2649, 0.2776, 0.4452, 0.5415,\n",
              "         0.6635],\n",
              "        [0.0293, 0.9290, 0.5949, 0.2514, 0.4324, 0.3504, 0.8079, 0.1742, 0.8765,\n",
              "         0.1987, 0.8959, 0.0633, 0.7901, 0.1861, 0.6697, 0.2353, 0.3265, 0.6423,\n",
              "         0.8318, 0.3070, 0.4478, 0.3538, 0.5577, 0.9482, 0.1761, 0.0157, 0.4176,\n",
              "         0.9934],\n",
              "        [0.3844, 0.2089, 0.5131, 0.3898, 0.5887, 0.4811, 0.2461, 0.9163, 0.6586,\n",
              "         0.6846, 0.6939, 0.8740, 0.5007, 0.3364, 0.8166, 0.4067, 0.1144, 0.0406,\n",
              "         0.0658, 0.0721, 0.1678, 0.2404, 0.2409, 0.8675, 0.9953, 0.4249, 0.7516,\n",
              "         0.0200],\n",
              "        [0.0594, 0.0201, 0.1409, 0.8706, 0.3985, 0.0205, 0.3438, 0.1074, 0.2141,\n",
              "         0.7378, 0.9037, 0.0473, 0.2190, 0.0516, 0.7549, 0.9224, 0.7707, 0.1909,\n",
              "         0.2626, 0.2952, 0.4900, 0.1868, 0.7012, 0.3677, 0.0378, 0.7422, 0.0340,\n",
              "         0.6640],\n",
              "        [0.0051, 0.7458, 0.3922, 0.8783, 0.3042, 0.5599, 0.7889, 0.8163, 0.0327,\n",
              "         0.2320, 0.7139, 0.4205, 0.4580, 0.3266, 0.4473, 0.9460, 0.4166, 0.8832,\n",
              "         0.5588, 0.8816, 0.8182, 0.6020, 0.3105, 0.3786, 0.9210, 0.7080, 0.7798,\n",
              "         0.1784],\n",
              "        [0.1502, 0.1868, 0.1028, 0.4787, 0.2750, 0.0036, 0.0683, 0.8683, 0.0836,\n",
              "         0.8379, 0.5702, 0.1009, 0.6301, 0.5391, 0.4438, 0.1088, 0.8498, 0.2000,\n",
              "         0.9508, 0.7271, 0.8220, 0.8570, 0.0832, 0.4618, 0.1216, 0.3571, 0.7831,\n",
              "         0.3557],\n",
              "        [0.3787, 0.0195, 0.5484, 0.0024, 0.9863, 0.3829, 0.4818, 0.9518, 0.3421,\n",
              "         0.0908, 0.0633, 0.0771, 0.8260, 0.4856, 0.2944, 0.6271, 0.3781, 0.9604,\n",
              "         0.5697, 0.3311, 0.5619, 0.8498, 0.8188, 0.8003, 0.2984, 0.0145, 0.9106,\n",
              "         0.6467],\n",
              "        [0.9206, 0.5403, 0.3794, 0.5008, 0.8054, 0.3536, 0.8261, 0.3610, 0.3007,\n",
              "         0.5515, 0.4696, 0.0643, 0.4193, 0.7747, 0.4761, 0.9773, 0.7099, 0.0794,\n",
              "         0.2351, 0.4547, 0.3927, 0.7732, 0.6807, 0.9422, 0.4799, 0.2460, 0.4858,\n",
              "         0.0129],\n",
              "        [0.1230, 0.6395, 0.3887, 0.6077, 0.4941, 0.7762, 0.5908, 0.8661, 0.8182,\n",
              "         0.4410, 0.0216, 0.3844, 0.2900, 0.2775, 0.7978, 0.4054, 0.5610, 0.4349,\n",
              "         0.8544, 0.6553, 0.6503, 0.7942, 0.2935, 0.8417, 0.0180, 0.1954, 0.0980,\n",
              "         0.6399],\n",
              "        [0.5232, 0.2494, 0.1682, 0.2834, 0.3076, 0.3186, 0.7293, 0.2030, 0.4650,\n",
              "         0.9137, 0.5245, 0.8516, 0.3577, 0.6773, 0.3407, 0.5917, 0.4051, 0.0305,\n",
              "         0.1571, 0.4488, 0.5483, 0.7707, 0.9620, 0.9122, 0.8319, 0.0707, 0.0322,\n",
              "         0.4513],\n",
              "        [0.5824, 0.7462, 0.5560, 0.6600, 0.3611, 0.9630, 0.1194, 0.1069, 0.7624,\n",
              "         0.9039, 0.7844, 0.7195, 0.3615, 0.4564, 0.4985, 0.7482, 0.9633, 0.2792,\n",
              "         0.5152, 0.8373, 0.2060, 0.2203, 0.9209, 0.1881, 0.6300, 0.4037, 0.9416,\n",
              "         0.8529],\n",
              "        [0.7933, 0.8566, 0.4432, 0.5105, 0.7020, 0.2396, 0.0704, 0.8629, 0.1839,\n",
              "         0.0567, 0.4668, 0.8552, 0.9192, 0.6083, 0.6498, 0.7321, 0.8720, 0.1557,\n",
              "         0.0097, 0.6025, 0.2183, 0.5391, 0.1795, 0.8771, 0.5619, 0.0145, 0.6105,\n",
              "         0.0107],\n",
              "        [0.6752, 0.0955, 0.6103, 0.5186, 0.9895, 0.0703, 0.7565, 0.8178, 0.7117,\n",
              "         0.9214, 0.4226, 0.8477, 0.5673, 0.6253, 0.8993, 0.0981, 0.3664, 0.3911,\n",
              "         0.8135, 0.6500, 0.5521, 0.3881, 0.2376, 0.5994, 0.2490, 0.7068, 0.2401,\n",
              "         0.2286],\n",
              "        [0.1144, 0.4579, 0.9508, 0.7004, 0.2703, 0.1752, 0.6344, 0.6530, 0.6894,\n",
              "         0.2814, 0.5427, 0.0582, 0.3506, 0.3557, 0.0348, 0.3478, 0.7583, 0.7820,\n",
              "         0.3101, 0.9875, 0.7254, 0.1207, 0.1480, 0.9635, 0.6867, 0.7059, 0.6312,\n",
              "         0.8657],\n",
              "        [0.3114, 0.2341, 0.5067, 0.6560, 0.1463, 0.5566, 0.8706, 0.3067, 0.4609,\n",
              "         0.5751, 0.6289, 0.7497, 0.0063, 0.4035, 0.7095, 0.4576, 0.5271, 0.7083,\n",
              "         0.2141, 0.8961, 0.8897, 0.9995, 0.4897, 0.0066, 0.2727, 0.0059, 0.3459,\n",
              "         0.6133],\n",
              "        [0.4243, 0.6360, 0.0642, 0.0571, 0.7039, 0.2038, 0.8813, 0.7037, 0.7703,\n",
              "         0.8727, 0.9719, 0.5195, 0.0746, 0.2210, 0.9045, 0.7461, 0.0062, 0.4244,\n",
              "         0.1136, 0.7364, 0.3112, 0.5126, 0.6290, 0.2629, 0.9335, 0.5703, 0.8451,\n",
              "         0.8442],\n",
              "        [0.8794, 0.3586, 0.5282, 0.4311, 0.0092, 0.7047, 0.2370, 0.9325, 0.6523,\n",
              "         0.6892, 0.9872, 0.7326, 0.0138, 0.8599, 0.2456, 0.2034, 0.6652, 0.0480,\n",
              "         0.8127, 0.4472, 0.7847, 0.5655, 0.3465, 0.8063, 0.9550, 0.0709, 0.1812,\n",
              "         0.7676],\n",
              "        [0.4464, 0.3834, 0.5752, 0.0991, 0.5118, 0.5205, 0.4210, 0.0898, 0.6452,\n",
              "         0.3574, 0.7523, 0.8593, 0.4060, 0.5359, 0.1275, 0.7594, 0.8785, 0.8765,\n",
              "         0.5618, 0.5859, 0.3411, 0.6017, 0.0959, 0.4344, 0.4813, 0.2661, 0.0199,\n",
              "         0.6640],\n",
              "        [0.7083, 0.0542, 0.2224, 0.3208, 0.6452, 0.0957, 0.9447, 0.0551, 0.2183,\n",
              "         0.0867, 0.2677, 0.7132, 0.9661, 0.8300, 0.9591, 0.0937, 0.5372, 0.5966,\n",
              "         0.8098, 0.9322, 0.4720, 0.9264, 0.6648, 0.4581, 0.5637, 0.9830, 0.1689,\n",
              "         0.0727],\n",
              "        [0.4511, 0.0752, 0.9306, 0.6271, 0.4290, 0.4116, 0.4222, 0.0300, 0.9633,\n",
              "         0.3888, 0.1847, 0.9108, 0.8670, 0.5239, 0.8353, 0.4958, 0.8878, 0.8423,\n",
              "         0.5250, 0.1617, 0.8328, 0.8336, 0.6065, 0.9241, 0.8779, 0.7628, 0.0445,\n",
              "         0.2328],\n",
              "        [0.5339, 0.6247, 0.2999, 0.9390, 0.9519, 0.9575, 0.7836, 0.9026, 0.3154,\n",
              "         0.9529, 0.6421, 0.3706, 0.3698, 0.0818, 0.5283, 0.0170, 0.6794, 0.0131,\n",
              "         0.1100, 0.1547, 0.6083, 0.7526, 0.4093, 0.3495, 0.9139, 0.6337, 0.8994,\n",
              "         0.1609],\n",
              "        [0.3825, 0.6596, 0.6951, 0.0942, 0.9878, 0.7907, 0.7536, 0.4683, 0.2076,\n",
              "         0.2781, 0.8305, 0.7921, 0.5474, 0.4397, 0.5361, 0.8053, 0.5606, 0.1868,\n",
              "         0.6576, 0.5776, 0.8824, 0.9665, 0.1793, 0.6396, 0.5256, 0.7221, 0.7926,\n",
              "         0.4310],\n",
              "        [0.5843, 0.9779, 0.8020, 0.4888, 0.3007, 0.5717, 0.0924, 0.8483, 0.2543,\n",
              "         0.2294, 0.0037, 0.6019, 0.1778, 0.2428, 0.1526, 0.3049, 0.2077, 0.9887,\n",
              "         0.9442, 0.5479, 0.8256, 0.9641, 0.5865, 0.5054, 0.3029, 0.0636, 0.9016,\n",
              "         0.9808],\n",
              "        [0.7269, 0.7538, 0.8868, 0.4221, 0.5526, 0.5764, 0.6973, 0.2395, 0.6397,\n",
              "         0.1582, 0.3167, 0.5636, 0.0310, 0.8042, 0.4105, 0.4796, 0.6945, 0.2895,\n",
              "         0.9573, 0.0938, 0.1153, 0.0862, 0.3111, 0.8119, 0.5160, 0.3445, 0.5336,\n",
              "         0.2408],\n",
              "        [0.0991, 0.3276, 0.4533, 0.9727, 0.0166, 0.9756, 0.1194, 0.6118, 0.1795,\n",
              "         0.2377, 0.4071, 0.8296, 0.1901, 0.3041, 0.2402, 0.6973, 0.3947, 0.6283,\n",
              "         0.0103, 0.0033, 0.2975, 0.4990, 0.5525, 0.0155, 0.3645, 0.0386, 0.0462,\n",
              "         0.8814]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ednYk0xQ4l9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "abf26164-10ed-4c93-f7a8-8755eeb2bbb1"
      },
      "source": [
        "output = net(X)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-705efee85c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-cf37bdb14f1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## Define a forward path for 'self' with input 'x'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [28 x 28], m2: [784 x 64] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB2xlIMvRKUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ec02c5b0-d05d-4caa-9545-8b54e25ea81e"
      },
      "source": [
        "X = X.view(-1, 28*28)   ## -1 is unknown shape , Be ready of any size of data. Same works for 1 also\n",
        "output = net(X)\n",
        "output"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.3133, -2.3437, -2.1856, -2.3869, -2.2510, -2.4336, -2.3536, -2.1858,\n",
              "         -2.3389, -2.2644]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ3P6F_tSG7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "ea7ca4be-f574-4ae9-96ed-170484278a8c"
      },
      "source": [
        "print(X.shape)\n",
        "plt.imshow(X.view(28,28))\n",
        "\n",
        "# Below is the image that we generated using the random data"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 784])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc685cda400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTHZCPOH/AyA3sDXdlu8LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUQHS5knTQrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## LETS PASS LABELED DATA AND MAKE THE MODEL RECOGNISE THE SAME : \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}